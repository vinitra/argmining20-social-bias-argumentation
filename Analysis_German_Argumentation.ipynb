{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacb818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da972f4",
   "metadata": {},
   "source": [
    "# Dataset 1a: German â†’ English Argumentation Reviews (Thiemo)\n",
    "\n",
    "1. Train a German GloVE model on these embeddings from column 'review'\n",
    "2. WEAT Analysis\n",
    "3. Word Co-occurence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf3878",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'background/'\n",
    "student_reviews_data = pd.read_excel(data_path + '201901_Studenten Reviews BI2 full.xlsx', header=1, index_col=0)\n",
    "student_reviews_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab05e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_review = ''.join(student_reviews_data['review'])\n",
    "with open(\"reviews_glove.txt\", \"w\") as text_file:\n",
    "    text_file.write(full_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12778a90",
   "metadata": {},
   "source": [
    "# Dataset 1b: German Argumentation Reviews (Thiemo)\n",
    "\n",
    "0. Translate text to English\n",
    "1. Train an English GloVE model on these embeddings from column 'review'\n",
    "2. WEAT Analysis\n",
    "3. Word Co-occurence analysis\n",
    "\n",
    "benefits: direct comparison to baselines from the original paper  \n",
    "cons: how much bias can be attributed to the content, and how much to the translator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deep_translator\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "translator = GoogleTranslator(source='de', target='en')\n",
    "\n",
    "def translate_5000(text):\n",
    "    characters = len(text)\n",
    "    print(characters > 3000)\n",
    "    translations = []\n",
    "    for i in np.arange(3000, characters, 3000):\n",
    "        translation = translator.translate(text[i-3000:i])\n",
    "        translations.append(translation)\n",
    "    translations.append(translator.translate(text[:-characters%3000]))\n",
    "    return ''.join(translations)\n",
    "\n",
    "student_reviews_data['review'] = student_reviews_data['review'].apply(lambda k: translate_5000(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ad936",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_reviews_data.to_csv('english_translated_student_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a4bcf",
   "metadata": {},
   "source": [
    "# Dataset 2: German Annotated Essays\n",
    "\n",
    "1. Train an German GloVE model on these embeddings from student essays\n",
    "2. WEAT Analysis\n",
    "3. Word Co-occurence analysis\n",
    "4. Analyze using annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'background/Coprus Coling/Annotations/'\n",
    "coling_essay_files = [file for file in os.listdir(data_path) if \".txt\" in file]\n",
    "essays = {}\n",
    "for essay_file in coling_essay_files:\n",
    "    with open(data_path+essay_file, \"r\") as myfile:\n",
    "        data = myfile.read().splitlines()\n",
    "        essays[int(essay_file.split('.txt')[0])] = data[1]\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92025af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = '\\n'.join(essays.values())\n",
    "with open(\"essays_glove.txt\", \"w\") as text_file:\n",
    "    text_file.write(all_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aba4b0",
   "metadata": {},
   "source": [
    "# Dataset 3: English Annotated Essays\n",
    "\n",
    "1. Train an English GloVE model on these embeddings from student essays\n",
    "2. WEAT Analysis\n",
    "3. Word Co-occurence analysis\n",
    "4. Analyze using annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b00c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'background/ArgumentAnnotatedEssays-2.0/brat-project-final/'\n",
    "eng_essay_files = [file for file in os.listdir(data_path) if \".txt\" in file]\n",
    "essays = {}\n",
    "for essay_file in eng_essay_files:\n",
    "    with open(data_path+essay_file, \"r\") as myfile:\n",
    "        data = myfile.read()\n",
    "        essays[int(essay_file.split('.txt')[0].split('essay')[1])] = data\n",
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d818fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_essays = '\\n'.join(essays.values())\n",
    "with open(\"eng_essays_glove.txt\", \"w\") as text_file:\n",
    "    text_file.write(all_essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a09ee6",
   "metadata": {},
   "source": [
    "# WEAT Co-occurence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0442f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python weat_cooccurrence_analysis.py \\\n",
    "    --data \"eng_essays_glove.txt\" \\\n",
    "    --output \"output/weat_cooccurrence_analysis\" \\\n",
    "    --processing_cores 7 \\\n",
    "    --tests 1 2 3 4 5 6 7 8 9 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
